{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-opsfusionlabs","title":"Welcome to OpsFusionlabs","text":"<p>opsfusionlabs is platform to learn about cloud computing, DevOps methodologies, and MLOps frameworks. Access valuable resources to boost your expertise and drive innovation in your projects. Start learning now! </p> <p>opsfusionlabs is platform to learn about cloud computing, DevOps methodologies, and MLOps frameworks. Access valuable resources to boost your expertise and drive innovation in your projects. Start learning now! </p>"},{"location":"Git/Branches/","title":"Git Branching","text":"<p>Git branches are effectively a pointer to a snapshot of your changes. When you want to add a new feature or fix a bug\u2014no matter how big or how small\u2014you spawn a new branch to encapsulate your changes. This makes it harder for unstable code to get merged into the main code base, and it gives you the chance to clean up your future's history before merging it into the main branch.</p>"},{"location":"Git/Branches/#git-branch","title":"Git Branch","text":"<ol> <li> <p>List all of the branches in your repository.     <pre><code>git branch --list \n</code></pre></p> </li> <li> <p>Create a new branch </p> <pre><code>git branch ofl-dev\n</code></pre> </li> <li> <p>Delete the specified branch</p> <pre><code>git branch -d ofl-dev\n</code></pre> </li> <li> <p>Force delete the specified branch, even if it has unmerged changes.</p> <pre><code>git branch -D ofl-dev\n</code></pre> </li> <li> <p>List all remote branches.</p> <pre><code>git branch -a \n</code></pre> </li> <li> <p>List all remote branches </p> <pre><code>git branch -r \n</code></pre> </li> <li> <p>Create and Switching to new branches</p> <pre><code>git checkout -b ofl-test\n</code></pre> </li> <li> <p>Switching branches</p> <pre><code>git checkout master\n</code></pre> </li> <li> <p>Checkout a remote branch     <pre><code>git\u00a0fetch --all\n</code></pre></p> <pre><code>git\u00a0checkout\u00a0\uff1cremotebranch\uff1e\n</code></pre> </li> </ol>"},{"location":"Git/Branches/#git-checkout","title":"Git Checkout","text":""},{"location":"Git/Git%20Credentials%20Setup/","title":"User Configuration","text":""},{"location":"Git/Git%20Credentials%20Setup/#setting-your-git-username-for-every-repository-on-your-computer","title":"Setting your Git username for every repository on your computer","text":"<ul> <li>Lets check the config list </li> </ul> <pre><code>git config --list\n</code></pre> <ul> <li>Set a Global Git username:</li> </ul> <pre><code>git config --global user.name \"&lt;GitHub Username&gt;\"\n</code></pre> <p>Output: </p> <p><pre><code>root@OpsFusionLabs-Git-Server:~/opsfusionlabs# git config --global user.name \"opsfusionlabs\"\nroot@OpsFusionLabs-Git-Server:~/opsfusionlabs# git config user.name\nopsfusionlabs\nroot@OpsFusionLabs-Git-Server:~/opsfusionlabs#\n</code></pre> - Set a Global Git user email:</p> <pre><code>git config --global user.email \"&lt;Git Hub Email ID&gt;\"\n</code></pre> <p>Output: </p> <p><pre><code>root@OpsFusionLabs-Git-Server:~/opsfusionlabs# git config --global user.email \"opsfusionlabs@gmail.com\"\nroot@OpsFusionLabs-Git-Server:~/opsfusionlabs# git config user.email\nopsfusionlabs@gmail.com\n</code></pre> - Confirm that we have set the Git username and email correctly:</p> <pre><code>git config user.name &amp;&amp; git config user.email\n</code></pre> <p>Output: </p> <pre><code>root@OpsFusionLabs-Git-Server:~/opsfusionlabs# git config user.name &amp;&amp; git config user.email\nopsfusionlabs\nopsfusionlabs@gmail.com\n</code></pre>"},{"location":"Git/Git%20Credentials%20Setup/#example","title":"Example:","text":"<pre><code>git config --global user.name \"opsfusionlabs\"\ngit config --global user.email \"opsfusionlabs@gmail.com\"\ngit config --list \n</code></pre>"},{"location":"Git/Git%20Merge/","title":"Git Merge","text":"<p>Merging is Git's way of putting a forked history back together again. The\u00a0<code>git merge</code>\u00a0command lets you take the independent lines of development created by\u00a0<code>git\u00a0branch</code>\u00a0and integrate them into a single branch.</p> <p>Note that all of the commands presented below merge into the current branch. The\u00a0current branch will be updated to reflect the merge, but the target branch will\u00a0be completely unaffected. Again, this means that\u00a0<code>git merge</code>\u00a0is often used in\u00a0conjunction with\u00a0<code>git checkout</code>for selecting the current branch and\u00a0<code>git\u00a0branch -d</code>\u00a0for deleting the obsolete target branch.</p>"},{"location":"Git/Git/","title":"Git","text":"<p>Git is the free and open source distributed version control system that's responsible for everything GitHub related that happens locally on your computer. This cheat sheet features the most important and commonly used Git commands for easy reference.</p>"},{"location":"Git/Workflow/","title":"Initial Git Work flow","text":""},{"location":"Git/Workflow/#repository","title":"Repository","text":""},{"location":"Git/Workflow/#initializing-a-repository-in-an-existing-directory","title":"Initializing a Repository in an Existing Directory","text":"<ul> <li> <p>Setting up a local repository</p> <pre><code>mkdir opsfusionlabs\ncd opsfusionlabs  \n</code></pre> <pre><code>git init\n</code></pre> </li> </ul> <p>Output: </p> <pre><code>root@OpsFusionLabs-Git-Server:~/opsfusionlabs# git init\nhint: Using 'master' as the name for the initial branch. This default branch name\nhint: is subject to change. To configure the initial branch name to use in all\nhint: of your new repositories, which will suppress this warning, call:\nhint:\nhint:   git config --global init.defaultBranch &lt;name&gt;\nhint:\nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\nhint: 'development'. The just-created branch can be renamed via this command:\nhint:\nhint:   git branch -m &lt;name&gt;\nInitialized empty Git repository in /root/opsfusionlabs/.git/\n</code></pre>"},{"location":"Git/Workflow/#cloning-an-existing-repository","title":"Cloning an Existing Repository","text":"<ul> <li> <p>You clone a repository with <code>git clone &lt;url&gt;</code> </p> <pre><code>git clone &lt;repository url&gt; \n</code></pre> </li> </ul>"},{"location":"Git/Workflow/#tracking-new-files","title":"Tracking New Files","text":""},{"location":"Git/Workflow/#git-status","title":"Git Status","text":"<ul> <li> <p>Git-status is used to understand what stage the files in a repository are at.</p> <pre><code>git status \n</code></pre> </li> </ul> <p>Output: </p> <pre><code>root@OpsFusionLabs-Git-Server:~/opsfusionlabs# git status\nOn branch master\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)\n</code></pre> <ul> <li> <p>Create new file\u00a0<code>app.py</code></p> <pre><code>touch app.py \n</code></pre> </li> </ul> <p>Output : </p> <pre><code>root@OpsFusionLabs-Git-Server:~/opsfusionlabs# touch app.py\nroot@OpsFusionLabs-Git-Server:~/opsfusionlabs#\nroot@OpsFusionLabs-Git-Server:~/opsfusionlabs# ll\ntotal 12\ndrwxr-xr-x 3 root root 4096 Jul 30 23:03 ./\ndrwx------ 6 root root 4096 Jul 30 22:59 ../\ndrwxr-xr-x 7 root root 4096 Jul 30 23:02 .git/\n-rw-r--r-- 1 root root    0 Jul 30 23:03 app.py\nroot@OpsFusionLabs-Git-Server:~/opsfusionlabs# git status\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        app.py\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n</code></pre>"},{"location":"Git/Workflow/#git-add","title":"Git add","text":"<ul> <li> <p>The\u00a0<code>git add</code>\u00a0command adds a change in the working directory to the staging area</p> <pre><code>git add app.py \n</code></pre> </li> </ul> <p>Output :</p> <pre><code>root@OpsFusionLabs-Git-Server:~/opsfusionlabs# git add app.py\nroot@OpsFusionLabs-Git-Server:~/opsfusionlabs# git status\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   app.py\n\nroot@OpsFusionLabs-Git-Server:~/opsfusionlabs#\n</code></pre>"},{"location":"Git/Workflow/#git-commit","title":"Git commit","text":"<ul> <li> <p><code>git commit</code>\u00a0creates a commit, which is like a snapshot of your repository. These commits are snapshots of your entire repository at specific times.</p> <pre><code>git commit -m \"commit message\" &lt;file name&gt;\n</code></pre> <pre><code>// stage all files in current directory\ngit add .\n\n// stage single file\ngit add &lt;file&gt;\n\n// stage multiple files\ngit add &lt;file1&gt; &lt;file2&gt; &lt;file3&gt; ...\n</code></pre> </li> </ul> <p>Output: </p> <pre><code>root@OpsFusionLabs-Git-Server:~/opsfusionlabs# git commit -m \"app.py file added\" .\n[master (root-commit) e473881] app.py file added\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 app.py\n</code></pre>"},{"location":"Git/Workflow/#git-diff","title":"Git diff","text":"<ol> <li> <p>Show difference between working directory and last commit.     <pre><code>git diff HEAD\n</code></pre></p> </li> <li> <p>Show difference between staged changes and last commit     <pre><code>git diff --cached\n</code></pre></p> </li> </ol>"},{"location":"Git/Workflow/#git-log","title":"Git log","text":"<ol> <li> <p>Check the Commit messages </p> <pre><code>git log \n</code></pre> </li> </ol> <p>Output:  <pre><code>root@OpsFusionLabs-Git-Server:~/opsfusionlabs# git log\ncommit e4738815ddb62b57d65e0bf560894f401416dd4e (HEAD -&gt; master)\nAuthor: opsfusionlabs &lt;opsfusionlabs@gmail.com&gt;\nDate:   Tue Jul 30 23:12:23 2024 +0530\n\n    app.py file added\nroot@OpsFusionLabs-Git-Server:~/opsfusionlabs#\n</code></pre></p> <ul> <li> <p>Limit number of commits by . E.g. \u201dgit log -5\u201d will limit to 5 commits.</p> <pre><code>git log -5\n</code></pre> </li> <li> <p>Condense each commit to a single line</p> <pre><code>git log --oneline\n</code></pre> </li> <li> <p>Display the full diff of each commit.</p> <pre><code>git log -p\n</code></pre> </li> <li> <p>Include which files were altered and the relative number of lines that were added or deleted from each of them</p> <pre><code>git log --stat\n</code></pre> </li> <li> <p>Search for commits by a particular author.</p> <pre><code>git log --author= \"opsfusionlabs\"\n</code></pre> </li> <li> <p>Search for commits with a commit message that matches <code>pattern</code> </p> <pre><code>git log --grep=\"&lt;pattern\"&gt;\n</code></pre> </li> <li> <p>Only display commits that have the specified file.  </p> <pre><code>git log -- app.py\n</code></pre> </li> <li> <p>graph flag draws a text based graph of commits on left side of commit msgs. --decorate adds names of branches or tags of commits shown  </p> <pre><code>git log --graph --decorate\n</code></pre> </li> </ul>"},{"location":"Git/Workflow/#undoing-changes","title":"Undoing Changes","text":"<ol> <li> <p>Create new commit that undoes all of the changes made in , then apply it to the current branch.</p> <pre><code>git revert &lt;commit id&gt;\n</code></pre> </li> <li> <p>Remove from the staging area, but leave the working directory unchanged. This unstages a file without overwriting any changes.</p> <pre><code>git reset &lt;file&gt; \n</code></pre> </li> <li> <p>Shows which files would be removed from working directory. Use the -f flag in place of the -n flag to execute the clean.</p> <pre><code>git clean -n\n</code></pre> </li> </ol>"},{"location":"Git/Workflow/#git-reset","title":"Git reset","text":"<ol> <li> <p>Reset staging area to match most recent commit, but leave the working directory unchanged.</p> <pre><code>git reset\n</code></pre> </li> <li> <p>Reset staging area and working directory to match most recent commit and overwrites all changes in the working directory.</p> <pre><code>git reset --hard\n</code></pre> </li> <li> <p>Move the current branch tip backward to , reset the staging area to match, but leave the working directory alone.</p> <pre><code>git reset &lt;commitId&gt;\n</code></pre> </li> <li> <p>Same as previous, but resets both the staging area &amp; working directory to match. Deletes uncommitted changes, and all commits after.</p> <pre><code>git reset --hard &lt;commitid&gt;\n</code></pre> </li> </ol>"},{"location":"Git/Workflow/#rewriting-git-history","title":"Rewriting git history","text":"<ol> <li> <p>Replace the last commit with the staged changes and last commit combined. Use with nothing staged to edit the last commit\u2019s message.</p> <pre><code>git commit --amend &lt;commit id&gt; \n</code></pre> </li> <li> <p>Rebase the current branch onto . can be a commit ID, branch name, a tag, or a relative reference to HEAD.</p> <pre><code>git rebase &lt;rebase&gt;\n</code></pre> </li> <li> <p>Show a log of changes to the local repository\u2019s HEAD. Add --relative-date flag to show date info or --all to show all refs.</p> <pre><code>git reflog \n</code></pre> </li> </ol>"},{"location":"Installations/Argo%20Cd/","title":"Installing Argo CD into EKS cluster","text":"<ol> <li>Create a name space called argocd <pre><code>kubectl create namespace argocd\n</code></pre> <pre><code>kubectl get ns \n</code></pre></li> <li>his command installs the latest stable version of Argo CD.</li> <li>By default Argo Cd come with cluster IP mode, we are going to change form cluster IP to Node port  </li> </ol>"},{"location":"Installations/Httpd%20Webserver/","title":"Apache Webserver Installation","text":""},{"location":"Installations/Httpd%20Webserver/#installing-apache","title":"Installing Apache","text":"<pre><code>sudo yum install httpd -y \nsudo systemctl start httpd \nsudo systemctl status httpd --no-pager\n</code></pre>"},{"location":"Installations/Install-Docker-Engine-on-RHEL/","title":"Docker Installation","text":""},{"location":"Installations/Install-Docker-Engine-on-RHEL/#install-docker-engine-on-rhel","title":"Install Docker Engine on RHEL","text":"<ol> <li> <p>Uninstall the old versions CRI  <pre><code>sudo yum remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-engine \\\n                  podman \\\n                  runc\n</code></pre></p> </li> <li> <p>Docker Installation</p> <ul> <li>2.1 Setup the Repository  <pre><code>sudo yum install -y yum-utils\n</code></pre> <pre><code>sudo yum-config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo\n</code></pre> <p>Note: If you want to can check the repo status --&gt;  <code>yum repolist</code> </p> </li> </ul> </li> <li> <p>Install Docker</p> <ul> <li>3.1 Install latest version of docker <pre><code>sudo yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin -y\n</code></pre> <p>Note:   Install specific version of docker <pre><code>sudo yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre></p> </li> </ul> </li> <li> <p>Start Docker <pre><code>sudo systemctl start docker\n</code></pre></p> </li> <li>Enable docker <pre><code>systemctl enable docker.service\n</code></pre></li> <li>Check docker status <pre><code>systemctl status docker.service --no-pager\n</code></pre></li> <li>Verify that the Docker Engine installation is successful by running the\u00a0<code>hello-world</code>\u00a0image. <pre><code>sudo docker run hello-world\n</code></pre> <pre><code>docker container ls \n</code></pre></li> <li>Docker Engine cleanup <ul> <li>8.1 How to remove  containers  <pre><code>docker container stop $(docker container ls -aq)\n\ndocker container rm $(docker container ls -aq) \n</code></pre></li> <li>8.2 How to remove docker images  <pre><code>docker rmi $(docker images -q) \n</code></pre></li> </ul> </li> </ol>"},{"location":"Installations/Jenkins%20Installation/","title":"Jenkins Installation","text":""},{"location":"Installations/Jenkins%20Installation/#manual-installation-on-rhel","title":"Manual Installation On RHEL","text":""},{"location":"Installations/Jenkins%20Installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Minimum hardware requirements:<ul> <li>256 MB of RAM</li> <li>1 GB of drive space (although 10 GB is a recommended minimum if running Jenkins as a Docker container)</li> </ul> </li> <li>Recommended hardware configuration for a small team:<ul> <li>4 GB+ of RAM</li> <li>50 GB+ of drive space</li> </ul> </li> </ul>"},{"location":"Installations/Jenkins%20Installation/#long-term-support-release","title":"Long Term Support release","text":"<ul> <li>Install Supporting packages <pre><code>sudo su - \nyum install wget tree -y\n</code></pre></li> <li>Setup the Jenkins stable Repository  <pre><code>sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo\n</code></pre> <pre><code>sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key\nsudo yum upgrade\n</code></pre></li> <li>Install JDK  <pre><code>sudo yum upgrade -y \n</code></pre> <pre><code>sudo yum install fontconfig java-17-openjdk -y\n</code></pre></li> <li>Install Jenkins and reload demon <pre><code>sudo yum install jenkins -y\n</code></pre> <pre><code>sudo systemctl daemon-reload\n</code></pre></li> </ul>"},{"location":"Installations/Jenkins%20Installation/#start-jenkins","title":"Start Jenkins","text":"<ol> <li>You can enable the Jenkins service to start at boot with the command <pre><code>sudo systemctl enable jenkins\n</code></pre></li> <li>You can start the Jenkins service with the command <pre><code>sudo systemctl start jenkins\n</code></pre></li> <li>You can check the status of the Jenkins service using the command <pre><code>sudo systemctl status jenkins --no-pager\n</code></pre></li> </ol>"},{"location":"Installations/Jenkins%20Installation/#installation-using-shell-script","title":"Installation Using Shell Script","text":""},{"location":"Installations/Jenkins%20Installation/#method-1","title":"Method 1","text":"<ol> <li>Create new file named as <code>jenkins.sh</code> <pre><code>touch jenkins.sh\n</code></pre></li> <li>Add the below code in jenkins.sh file <pre><code>vi jenkins.sh \n</code></pre> <pre><code>#/bin/bash\n#Author: Sankeerth Chillamcharla\n#Orgination: OpsfusionLabs\n\n\nsudo yum install wget tree -y\nsudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo\nsudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key\nsudo yum upgrade -y\nsudo yum install fontconfig java-17-openjdk -y\nsudo yum install jenkins -y\nsudo systemctl daemon-reload\nsudo systemctl enable jenkins\nsudo systemctl start jenkins\nsudo systemctl status jenkins --no-pager\n</code></pre> <pre><code>:wq!\n</code></pre></li> <li>Change the file permission to make as a executable file <pre><code>chmod +x jenkins.sh\n</code></pre></li> <li>Run Shell Script <pre><code>sh jenkins.sh \n</code></pre></li> </ol>"},{"location":"Installations/Jenkins%20Installation/#method-2","title":"Method 2","text":"<ol> <li>Clone git repo </li> <li>Change the file permission to make as a executable file</li> <li>Run Shell Script</li> </ol>"},{"location":"Installations/Jenkins%20Installation/#post-installation-setup-wizard","title":"Post-installation setup wizard","text":"<ul> <li>Browse to\u00a0<code>http://localhost:8080</code>\u00a0(or whichever port you configured for Jenkins when installing it) and wait until the\u00a0Unlock Jenkins\u00a0page appears.</li> </ul> <ul> <li> <p>The command:\u00a0<code>sudo cat /var/lib/jenkins/secrets/initialAdminPassword</code>\u00a0will print the password at console.</p> </li> <li> <p>Install suggested plugins\u00a0- to install the recommended set of plugins, which are based on most common use cases.</p> </li> <li> <p>When the\u00a0Create First Admin User\u00a0page appears, specify the details for your administrator user in the respective fields and click\u00a0Save and Finish.</p> </li> </ul>"},{"location":"Installations/Docker/Managing%20Docker%20Containers/","title":"Managing Docker Containers","text":"<p>Introduction: </p> <p>Docker provides the ability to package and run an application in an isolated environment called a container. Container management is a process for automating the creation, deployment and scaling of containers.\u00a0</p> <p>Container management facilitates the addition, replacement and organization of containers on a large scale</p> <ol> <li> <p>Lets list the container if any container is running  <pre><code>docker container ls -a\n</code></pre></p> </li> <li> <p>Create a nginx container <pre><code> docker container create nginx\n</code></pre></p> </li> <li>create a container and name it as ofl01  <pre><code>docker container create --name ofl01 nginx\n</code></pre></li> <li>Start the container which is created <pre><code>docker container start ofl01\n</code></pre></li> <li>Create a new container using run option <pre><code>docker container run --name server01 centos\n</code></pre></li> <li> <p>Create a new container by supplying (-i and -t) option to get interactive terminal, by executing the below command, Note: \u201c-i\u201d stands for interactive &amp; \u201c-t\u201d stands for terminal. So we are requesting an interactive terminal for a container. <pre><code>docker container run --name server02 -i -t centos\n</code></pre></p> <p>Note: The container is created and interactive terminal is displayed. You can run commands inside the container and exit out.</p> </li> <li> <p>Create a new container with detached interactive terminal (-d -i -t) option, by executing the below command.     Note: d stand detached mode. <pre><code>docker container run --name server03 -dit centos\n</code></pre></p> </li> <li>Attach the container server03 <pre><code>docker container attach server03\n</code></pre></li> <li>Run a command inside the container without attaching to the container by using exec option. <pre><code>docker container exec server03 cat /etc/resolv.conf\n</code></pre></li> <li>Rename the existing container <pre><code>docker container rename server03 server003\n</code></pre></li> <li>Pause the existing container <pre><code>docker container pause server003\n</code></pre></li> <li>Unpause the existing container <pre><code>docker container unpause server003\n</code></pre></li> <li>Stop the existing container <pre><code>docker container stop server003\n</code></pre></li> <li>Start the existing container  <pre><code>docker container start server003\n</code></pre></li> <li>Restart the existing container <pre><code>docker container restart server003\n</code></pre></li> <li>Kill the container <pre><code>docker container kill server003\n</code></pre></li> </ol>"},{"location":"Kubernetes/K8%20Projects/","title":"K8 Projects","text":"<p>List of Projects </p> <ol> <li>Creating and Testing a Horizontal Pod Autoscaling (HPA) in Kubernetes Cluster</li> </ol>"},{"location":"Kubernetes/Storage/","title":"Storage","text":"<p>Kubernetes (k8s) offers a flexible and scalable storage management system for containers running within a cluster. The storage in Kubernetes can be broadly categorized into two types: ephemeral (temporary) storage and persistent storage.</p>"},{"location":"Kubernetes/Storage/#1-ephemeral-storage","title":"1. Ephemeral Storage:","text":"<p>Ephemeral storage is tied to the lifecycle of a Pod. When a Pod is deleted, the storage is also deleted. This type of storage is suitable for use cases where data doesn't need to persist after the Pod is terminated. emptyDir</p> <ul> <li>emptyDir: As mentioned earlier, <code>emptyDir</code> is a temporary directory that is created when a Pod is assigned to a Node and deleted when the Pod is removed.</li> <li>ConfigMap and Secret: These are used to store configuration files and sensitive data like passwords, respectively. They can be mounted as volumes, but the data is not meant to persist beyond the Pod's lifecycle.</li> <li>Downward API: Provides metadata about the Pod or the node where the Pod is running, available as a volume.</li> </ul>"},{"location":"Kubernetes/Storage/#2-persistent-storage","title":"2. Persistent Storage:","text":"<p>Persistent storage, on the other hand, is designed to outlive the Pod that uses it. It allows data to be retained across Pod restarts, enabling stateful applications to function effectively in Kubernetes.</p>"},{"location":"Kubernetes/Storage/#persistent-volumes-pvs-and-persistent-volume-claims-pvcs","title":"Persistent Volumes (PVs) and Persistent Volume Claims (PVCs):","text":"<ul> <li> <p>Persistent Volume (PV): A Persistent Volume is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using StorageClasses. It is independent of the Pod's lifecycle, meaning the data stored in a PV can survive after the Pod is deleted.</p> </li> <li> <p>Persistent Volume Claim (PVC): A PVC is a request for storage by a user. It is similar to a Pod requesting CPU or memory resources. PVCs are bound to PVs, and they can request specific storage size, access modes (e.g., ReadWriteOnce, ReadOnlyMany, ReadWriteMany), and storage class.</p> </li> </ul>"},{"location":"Kubernetes/Storage/#common-storage-solutions-in-kubernetes","title":"Common Storage Solutions in Kubernetes:","text":"<ol> <li> <p>NFS (Network File System): NFS is a distributed file system that allows multiple nodes to share storage. It is suitable for ReadWriteMany access mode, where multiple Pods across different nodes can read and write data simultaneously.</p> </li> <li> <p>Cloud Storage:</p> <ul> <li>AWS EBS (Elastic Block Store): A block storage service provided by AWS that can be used as a PV in Kubernetes. It supports ReadWriteOnce, meaning it can be attached to a single Pod at a time.</li> <li>GCP Persistent Disks: Similar to AWS EBS but provided by Google Cloud Platform. It supports ReadWriteOnce.</li> <li>Azure Disks and Azure Files: Azure Disks provide block storage, while Azure Files offer file storage that can be shared across multiple Pods.</li> <li>CSI (Container Storage Interface): CSI is an industry-standard that Kubernetes uses to interface with various storage solutions. CSI drivers can be used to manage storage for a wide range of environments, from cloud-based storage to on-premises solutions.</li> </ul> </li> <li> <p>Ceph: A distributed storage system that provides object, block, and file storage. Ceph can be integrated with Kubernetes using the Rook operator, which allows Kubernetes to manage Ceph clusters and use Ceph-based storage.</p> </li> <li> <p>GlusterFS: Another distributed file system that allows for scalable storage in Kubernetes.</p> </li> </ol>"},{"location":"Kubernetes/Working%20with%20Deployments/","title":"Working with Deployments","text":"<p>Deployments in Kubernetes is a crucial part of managing applications in a cluster. Deployments provide a higher-level abstraction over ReplicaSets, offering additional features like rolling updates, rollbacks, and easier scaling. Here\u2019s a guide to understanding and working with Kubernetes Deployments </p>"},{"location":"Kubernetes/Working%20with%20Deployments/#why-deployment","title":"Why Deployment ?","text":"<p>A Deployment is a Kubernetes resource that manages a ReplicaSet and provides declarative updates to applications. It ensures that a specified number of pod replicas are running at any given time and can automatically roll out changes to your application in a controlled way.</p>"},{"location":"Kubernetes/Working%20with%20Deployments/#key-features","title":"Key Features","text":"<ul> <li>Rolling Updates: Gradually update pods with new versions while maintaining application availability.</li> <li>Rollbacks: Revert to a previous stable version if something goes wrong during an update.</li> <li>Declarative Updates: Simply update the Deployment configuration, and Kubernetes handles the rest.</li> <li>Scaling: Easily scale the number of replicas up or down.</li> <li>Self-Healing: Automatically replaces failed pods to maintain the desired state.</li> </ul>"},{"location":"Kubernetes/Working%20with%20Deployments/#imperative-method","title":"Imperative Method","text":"<ul> <li> <p>Create Deployment using <code>nginx:latest</code>  image</p> <pre><code>kubectl create deployment nginx-deployment --image=nginx:latest\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Deployments/#inspecting-deployments","title":"Inspecting Deployments","text":"<ul> <li>Describe deployment</li> </ul> <pre><code>kubectl describe deployment nginx-deployment \n</code></pre> <pre><code>kubectl get all\n</code></pre> <ul> <li>Delete Deployment </li> </ul> <pre><code>kubectl delete deployment nginx-deployment \n</code></pre>"},{"location":"Kubernetes/Working%20with%20Deployments/#auto-generate-mainfest-files","title":"Auto Generate Mainfest Files","text":"<ul> <li> <p>Create deployment of <code>httpd</code> web server with 5 replicas in yaml </p> <pre><code>kubectl create deployment httpd-deployment --image=httpd --replicas=5 --dry-run=client -o yaml | tee httpd-deployment.yml\n</code></pre> </li> <li> <p>Deploy the <code>httpd-deployment.yml</code> main file </p> <pre><code>kubectl apply -f httpd-deployment.yml\n</code></pre> <pre><code>kubectl get all\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Deployments/#declarative-method","title":"Declarative Method","text":"<ul> <li> <p>Example Configuration of <code>nginx</code> deployment file </p> Deployments/ofl-deployment.yml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ofl-nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ofl-nginx-webserver\n  template:\n    metadata: \n      name: ofldeploymnet\n      labels:\n        app: ofl-nginx-webserver\n        env: production\n        tier: front-end\n    spec:\n      containers:\n      - name: ofl-nginx-container\n        image: nginx:latest\n        ports:\n          - containerPort: 80\n</code></pre> <p>Explanation:</p> <ul> <li>replicas: Specifies that three pod replicas should be running.</li> <li>selector: Matches pods with the label `app: ofl-nginx-webserver.</li> <li> <p>template: Defines the pod specification, including container images and other configurations.</p> </li> <li> <p>Create `ofl-deployment</p> <pre><code>kubectl apply -f ofl-deployment.yml\n</code></pre> </li> <li> <p>List of the Deployments</p> <pre><code>kubectl get deployment -o wide\n</code></pre> </li> <li> <p>Describe Deployments</p> <pre><code>kubectl describe deployment ofl-nginx-deployment\n</code></pre> </li> <li> <p>List out the all objects </p> <pre><code>kubectl get all \n</code></pre> </li> </ul> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Deployments/#perform-scaling-operations","title":"Perform scaling operations","text":"<p>Manually scaling the number of replicas in a Deployment is easy. You can do it imperatively with the <code>kubectl</code> scale command, or declaratively by updating the YAML file and re-posting to the API server. You\u2019ll do it both ways, but the preferred way is the declarative way.</p> <ul> <li> <p>First Verify the current number of replicas.</p> <pre><code>kubectl get deploy ofl-nginx-deployment\n</code></pre> </li> <li> </li> <li> </li> <li> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Deployments/#scale-up","title":"Scale Up","text":"<ul> <li> <p>Run the following command to scale up to 5 pods and verify the operation.</p> <pre><code>kubectl scale deploy ofl-nginx-deployment --replicas 5\n</code></pre> <pre><code>kubectl get deploy ofl-nginx-deployment\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Deployments/#scale-down","title":"Scale Down","text":"<ul> <li> <p>Run the following command to scale Down to 3 pods and verify the operation.</p> <pre><code>kubectl scale deploy ofl-nginx-deployment --replicas 3\n</code></pre> <pre><code>kubectl get deploy ofl-nginx-deployment\n</code></pre> </li> <li> <p>Run the following command to list the replicates </p> <pre><code>kubectl get rs \n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Deployments/#self-healing","title":"Self Healing","text":"<ul> <li> <p>Run the following command to list the pods</p> <pre><code>kubectl get pods \n</code></pre> </li> <li> <p>Run the following command to delete pods </p> <pre><code>kubectl delete pod --all\n</code></pre> </li> <li> <p>Run the following command to list the pods</p> <pre><code>kubectl get pods \n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Deployments/#perform-a-rolling-update","title":"Perform a rolling update","text":"<ul> <li> <p>To perform a rolling update on the app already deployed. We\u2019ll assume the new version of the app has already been created and containerized as an image with the 2.0 tag. </p> </li> <li> <p>When we \u201cupdate\u201d a Pod, we\u2019re actually terminating it and replacing it with a new one. Pods are designed as immutable objects, so we never change or update existing ones.</p> </li> <li> <p>Update the image version in the Deployment\u2019s resource manifest. The initial release of the app is using the <code>httpd:2.4.6</code> image. Update that to reference the newer <code>httpd:latest</code> mage and save your changes. This ensures next time the manifest is posted to the API server, all Pods managed by the Deployment will be replaced with new ones running the new 2.0 image.</p> </li> </ul> <p>Terminology: We often use the terms update, rollout, and release to mean the same thing \u2013 issuing a new version of an app.</p> <ul> <li> <p>Example Configuration of <code>httpd:1.0</code> deployment file </p> Deployments/ofl-rolling-update.yml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ofl-httpd-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ofl-httpd-deployment\n  revisionHistoryLimit: 5\n  progressDeadlineSeconds: 300\n  minReadySeconds: 10\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 1\n  template:\n    metadata: \n      name: ofldeploymnet\n      labels:\n        app: ofl-httpd-deployment\n        env: production\n        tier: front-end\n    spec:\n      containers:\n      - name: ofl-httpd-container\n        image: httpd:1.0\n        ports:\n          - containerPort: 80\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n</code></pre> </li> <li> <p><code>revisionHistoryLimit</code> tells Kubernetes to keep the configs of the previous 5 releases. This means the previous 5 ReplicaSet objects will be kept and you can rollback to any of them.</p> </li> <li> <p><code>progressDeadlineSeconds</code> governs how long Kubernetes waits for each new Pod replica to start, before considering the rollout to have stalled. This config gives each Pod replica its own 5 minute window to come up.</p> </li> <li> <p>.<code>spec.minReadySeconds</code> throttles the rate at which replicas are replaced. The one in the example tells Kubernetes that any new replica must be up and running for 10 seconds, without any issues, before it\u2019s allowed to update/replace the next one in sequence. Longer waits give you a chance to spot problems and avoid updating all replicas to a dodgy version. In the real world, you should make the value large enough to trap common failures.</p> <ul> <li> <p><code>.spec.strategy</code></p> <ul> <li>Update using the <code>RollingUpdate</code> strategy</li> <li>Never have more than one Pod below desired state (maxUnavailable: 1)</li> <li>Never have more than one Pod above desired state (maxSurge: 1)</li> </ul> </li> <li> <p>Run the following command to create <code>ofl-rolling-update</code> deployment </p> <pre><code>kubectl apply -f ofl-rolling-update.yml\n</code></pre> </li> <li> <p>Check rollout history of the <code>ofl-httpd-deployment</code></p> <pre><code>kubectl rollout history deployment ofl-httpd-deployment\n</code></pre> Output of kubectl rollout history deployment ofl-httpd-deployment<pre><code>deployment.apps/ofl-httpd-deployment \nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n</code></pre> </li> <li> <p>Check the rollout status      <pre><code>kubectl rollout status deployment ofl-httpd-deployment\n</code></pre></p> </li> <li> <p>Pausing and resuming rollouts</p> <pre><code>kubectl rollout pause deploy ofl-httpd-deployment\n</code></pre> <pre><code>kubectl rollout resume deploy ofl-httpd-deployment\n</code></pre> </li> <li> <p>Once it completes, you can verify with<code>kubectl</code>get deploy</p> <pre><code>kubectl get deploy ofl-httpd-deployment\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Deployments/#rollout-a-deployment","title":"Rollout a Deployment","text":"<ul> <li> <p>Now deploy the new image with tag <code>2.0</code></p> Deployments/ofl-rolling-update.yml<pre><code>    spec:\n      containers:\n      - name: ofl-httpd-container\n        image: httpd:2.0 # Updating from latest to version 1\n        ports:\n          - containerPort: 80\n        resources:\n</code></pre> </li> <li> <p>Run the following command to update <code>ofl-rolling-update</code> deployment  </p> <pre><code>kubectl apply -f ofl-rolling-update.yml\n</code></pre> </li> <li> <p>Run the following command to check rollout history</p> <pre><code>kubectl rollout history deployment ofl-httpd-deployment\n</code></pre> </li> </ul> <p>Revision 1 was the initial deployment that used the 1.0 image tag. Revision 2 is the rolling update you just performed.</p>"},{"location":"Kubernetes/Working%20with%20Deployments/#rolling-back-a-deployment","title":"Rolling Back a Deployment","text":"<ul> <li> <p>If something goes wrong after an update, you can roll back the Deployment to a previous revision.</p> <p><pre><code>kubectl rollout undo deployment ofl-httpd-deployment  --to-revision=1\n</code></pre>  or  </p> <pre><code>kubectl rollout undo deployment/ofl-httpd-deployment`\n</code></pre> <p>This command rolls back the Deployment to the previous stable version.</p> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Deployments/#clean-up","title":"Clean up","text":"<ul> <li> <p>Run the following command to clear all deployment in cluster</p> <pre><code>kubectl delete all --all\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Namesapce/","title":"Working with Namespace","text":"<p>Namespaces are a native way to divide a single Kubernetes cluster into multiple virtual clusters.</p> <ul> <li> <p>Run the following command to list all Kubernetes API resources</p> <pre><code>kubectl api-resources\n</code></pre> </li> <li> <p>Namespaces a good way of sharing a single cluster among different departments and environments. For example, </p> </li> <li> <p>A single cluster might have the following Namespaces.</p> <ul> <li>Dev</li> <li>Test</li> <li>QA</li> </ul> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Namesapce/#creating-namespaces","title":"Creating Namespaces","text":"<ul> <li> <p>Run the following command to create namespace called <code>opsfulsionlabs</code></p> <pre><code>kubectl create ns opsfulsionlabs\n</code></pre> </li> </ul> <p>Declaratively</p> <ul> <li> <p>Run the following command to create namespace called <code>opsfulsionlabs</code></p> Namespace/ofl-namespace.yml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: opsfusionlabs\n  labels:\n    name: OpsFusionLabs\n    env: dev\n</code></pre> <pre><code>kubectl apply -f ofl-namespace.yml\n</code></pre> </li> <li> <p>Auto Generate Namespace</p> <ul> <li> </li> <li> </li> </ul> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Namesapce/#imperatively","title":"Imperatively","text":""},{"location":"Kubernetes/Working%20with%20Namesapce/#ns-mainfest-file-in-yml","title":"Ns mainfest file in YML","text":"<pre><code>kubectl create namespace opsfusionlabs-yml -o yaml | tee ofl-namespace-autogenerate.yml\n</code></pre>"},{"location":"Kubernetes/Working%20with%20Namesapce/#ns-mainfest-file-in-json","title":"Ns mainfest file in\u00a0JSON","text":"<pre><code>kubectl create namespace opsfusionlabs-json -o json | tee ofl-namespace-autogenerate.json\n</code></pre>"},{"location":"Kubernetes/Working%20with%20Namesapce/#inspecting-namespaces","title":"Inspecting Namespaces","text":"<ul> <li> <p>Run the following command to list all namespaces</p> <pre><code>kubectl get namespaces \n</code></pre> <pre><code>kubectl get ns \n</code></pre> </li> <li> <p>Run the following command to describe namespaces</p> <pre><code>kubectl describe ns deafult \n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Namesapce/#deleting-namespace","title":"Deleting Namespace","text":"<ul> <li> <p>Imperatively</p> <ul> <li> <p>Run the following command to delete namespaces </p> <pre><code>kubectl delete ns &lt;name od the namespace&gt; \n</code></pre> </li> </ul> </li> <li> <p>Declaratively</p> <ul> <li> <p>Run the following command to delete namespaces </p> <pre><code>kubectl delete -f ofl-namespace.yml\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Namesapce/#set-new-default-namespace","title":"Set New Default Namespace","text":"<p>When you start using Namespaces, you\u2019ll quickly realize it\u2019s painful remembering to add the <code>-n</code> or <code>--namespace</code> flag on all <code>kubectl</code> commands. A better way might be to set your <code>kubeconfig</code> to automatically work with a particular Namespace.</p> <ul> <li> <p>Run the following command to change namespaces default to opsfusionlabs</p> <pre><code>kubectl config set-context --current --namespace opsfusionlabs\n</code></pre> </li> <li> <p>Run the following command to list the objects in opsfusionlabs namespace</p> <pre><code>kubectl get all \n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Namesapce/#mapping-objects-in-specific-namespace","title":"Mapping Objects in Specific Namespace","text":""},{"location":"Kubernetes/Working%20with%20Namesapce/#imperatively_1","title":"Imperatively","text":"<ul> <li> <p>Run the following command to create nginx pod in  <code>opsfusionlabs</code> namespace </p> <pre><code>kubectl run ofl-imp-nginx --image=ingnix:latest -n opsfusionlabs\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Namesapce/#declaratively","title":"Declaratively","text":"<ul> <li> <p>Run the following command to create nginx pod in  <code>opsfusionlabs</code> namespace </p> <pre><code>kubectl apply -f ofl-mapping-ns.yml\n</code></pre> Namespace/ofl-mapping-ns.yml<pre><code>apiVersion: v1\nkind: Pod\nmetadata: \n  name: ofl-ns-mapping\n  labels:\n    name: PodwithPort\n    environment: production\n    tier: front-end\n  namespace: opsfusionlabs\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Namesapce/#clean-up","title":"Clean up","text":"<ul> <li> <p>Run the following command to clear all deployment in cluster</p> <pre><code>kubectl delete all --all -n opsfusionlabs\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Pods/","title":"Working with Pods","text":""},{"location":"Kubernetes/Working%20with%20Pods/#introduction","title":"Introduction","text":"<p>Pods are the Smallest Deployable Units of computing that can be created and managed in Kubernetes. Pods with super-powers such as self-healing, scaling, updates and rollbacks.  \u00a0  \u00a0 Some quick examples If we want to deploy an app, you deploy it in a Pod. If you terminate an app, you terminate its Pod. If you scale an app up or down, you add or remove Pods.</p> <p>Why Pods ? </p> <ol> <li>Write your app/code </li> <li>Package it as a container image </li> <li>Wrap the container image in a Pod</li> <li>Run it on Kubernetes</li> </ol> <p>Note:  Kubernetes doesn\u2019t allow containers to run directly on a cluster, they always have to be wrapped in a Pod.</p> <ol> <li>Pods augment containers</li> <li>Pods assist in scheduling</li> <li>Pods enable resource sharing</li> </ol> <p>Pods augment containers </p> <ul> <li>Labels and annotations</li> <li>Restart policies</li> <li>Probes (startup probes, readiness probes, liveness probes, and potentially more)</li> <li>Affinity and anti-affinity rules</li> <li>Termination control</li> <li>Security policies</li> <li>Resource requests and limits</li> </ul> <p>Deploy Pod using Imperative method Deploy a Pod using Declarative method Generate the pod manifest in the YAML and JSON format Reading the Pod's information and metadata Listing the objects output in JSON or YAML format Entering into a container in a Pod Static Pod Multi-Container Pod Sidecar Container Deleting a Pod</p>"},{"location":"Kubernetes/Working%20with%20Pods/#imperative-method","title":"Imperative Method","text":"<ol> <li> <p>Create a pod with Imperative Method</p> <pre><code>kubectl run &lt;desired-pod-name&gt; --image &lt;Container-Image&gt; --generator=run-pod\n</code></pre> <pre><code>kubectl run webserver --image nginx\n</code></pre> </li> </ol>"},{"location":"Kubernetes/Working%20with%20Pods/#introspecting-running-pods","title":"Introspecting running Pods","text":"<ol> <li> <p>List the pod status</p> <pre><code> kubectl get pods\n</code></pre> </li> <li> <p>List Pods with wide option -o wide gives a couple more columns but is still a single line of output</p> <pre><code>kubectl get pods -o wide\n</code></pre> </li> <li> <p>Get complete pod in details </p> <pre><code>kubectl describe pod &lt;pod name&gt;\n</code></pre> <pre><code>kubectl describe pod webserver\n</code></pre> </li> <li> <p>Deleting a Pod </p> <pre><code>kubectl delete &lt;pod name&gt;\n</code></pre> <pre><code>kubectl delete pod webserver\n</code></pre> </li> </ol>"},{"location":"Kubernetes/Working%20with%20Pods/#declarative-method","title":"Declarative Method","text":"<p>For the declarative method, we use YAML (Yet Another Markup Language) files to present the configuration of what we desire to be the end product.</p> <p>Run a kubectl explain pods command to list all possible Pod attributes.  Beware, the command returns over 1,000 lines and the following output has been trimmed.</p> <pre><code>kubectl explain pods --recursive\n</code></pre> <pre><code>kubectl explain pod.spec.restartPolicy\n</code></pre>"},{"location":"Kubernetes/Working%20with%20Pods/#pod-manifest-files","title":"Pod manifest files","text":"<p>In order to create a pod using a declarative method, we need to create a yaml file and prepare a template. \u00a0 Example Configuration</p> <pre><code>---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: firstpod\nspec:\n  containers:\n    - name: nginx\n      image: nginx:latest\n</code></pre> <ul> <li> <p>The .apiVersion defines the schema version to use when creating the object. This file is defining a Pod object and telling Kubernetes to build it using the v1 Pod schema.</p> <ul> <li> <p>The normal format for apiVersion is api-group/version</p> </li> <li> <p>However, Pods are defined in a special API group called the core group which omits the api-group part. </p> </li> <li> <p>For example, StorageClass objects are defined in the v1 schema of the storage.k8s.io API group and are described in YAML files as storage.k8s.io/v1. </p> </li> </ul> </li> <li> <p>The .kind field tells Kubernetes the type of object being defined. This file is defining a Pod object.</p> </li> <li>The .metadata section is where you attach things such as names, labels, annotations, and a Namespace.</li> <li>The .spec section is where you define the containers the Pod will run. This is called the Pod template, and this example is defining a single-container Pod based on the nginx:latest image.</li> </ul>"},{"location":"Kubernetes/Working%20with%20Pods/#dry-run","title":"Dry Run","text":"<p>A dry run is a way to simulate the execution of a command without actually applying any changes.</p> <p>You can use the <code>--dry-run</code> flag with various <code>kubectl</code> commands to perform a dry run. As of Kubernetes 1.18 and later, the <code>--dry-run</code> flag has been updated to take two options: <code>client</code> and <code>server</code>.</p> <ul> <li> <p><code>--dry-run=client</code>: This performs the dry run on the client side without sending the request to the API server. This was the default behavior in Kubernetes 1.17 and earlier.</p> </li> <li> <p><code>--dry-run=server</code>: This performs the dry run on the server side, meaning it sends the request to the API server, which then validates the request and simulates the changes without persisting them. This is the preferred option in newer versions, as it provides more accurate validation.</p> </li> </ul> <pre><code>kubectl run webserver --image nginx --dry-run=client\n</code></pre> <p>Dry Run Benefits</p> <ul> <li>Validation: Ensure your manifests are correct and will be accepted by the API server.</li> <li>Safety: Avoid unintended changes to the live environment.</li> <li>Automation: Use dry runs as part of CI/CD pipelines to validate configurations before deploying them.</li> </ul>"},{"location":"Kubernetes/Working%20with%20Pods/#auto-generate-pod-manifest-files","title":"Auto Generate Pod manifest files","text":"<p>we can generate the pod manifest in two different formats </p> <ol> <li>YML</li> <li>Json</li> </ol>"},{"location":"Kubernetes/Working%20with%20Pods/#pod-manifest-in-yml","title":"Pod manifest in yml","text":"<pre><code>kubectl run webserver1 --image nginx --dry-run=client -o yaml | tee webserver1.yml\n</code></pre> <ul> <li> <p>Create a pod containing a nginx server using webserver1.yml file </p> <pre><code>kubectl apply -f webserver1.yml\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Pods/#pod-manifest-in-json","title":"Pod manifest in Json","text":"<pre><code>kubectl run webserver2  --image httpd --dry-run=client -o json | tee webserver2.json\n</code></pre> <ul> <li> <p>Create a pod containing a httpd server using webserver2.json file </p> <pre><code>kubectl apply -f webserver2.json\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Pods/#labels","title":"Labels","text":"<p>labels are key-value pairs that are attached to objects, such as Pods, Services, and Nodes. Labels are used to organize and select subsets of objects. They are a powerful way to manage and query resources in your Kubernetes cluster.</p> <ul> <li> <p>Key: A string that identifies the label. Keys can be up to 63 characters long, must begin and end with an alphanumeric character, and can include dashes (<code>-</code>), underscores (<code>_</code>), and dots (<code>.</code>). Optionally, keys can be prefixed with a domain name followed by a slash, allowing for more organizational control (e.g., <code>app.kubernetes.io/name</code>).</p> </li> <li> <p>Value: A string that is associated with the key. Values can be up to 63 characters long and must adhere to the same character rules as keys.</p> </li> <li> <p>Selectors: Labels are often used in conjunction with selectors to query or filter resources. For example, you can create a Service that selects all Pods with a specific label.</p> </li> </ul> <p>Example Configuration </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: podwithlabel\n  labels:\n    name: PodwithLabel\n    environment: production\n    tier: front-end\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n</code></pre> <ul> <li> <p>Create Pod without label: </p> <pre><code>kubectl apply -f 01-pod.yml\n</code></pre> </li> <li> <p>Create pod with labels </p> <pre><code>kubectl apply -f 02-PodwithLabel.yml\n</code></pre> </li> <li> <p>list pods with labels </p> <pre><code>kubectl get pods --show-labels \n</code></pre> </li> <li> <p>Add or override the labels to the existing pods </p> <pre><code>kubectl label pod webserver2 environment: production\n</code></pre> </li> <li> <p>Listing Pods with a Specific Label</p> <pre><code>kubectl get pods -l environment=production\n</code></pre> </li> <li> <p>Delete pod with label </p> <pre><code>kubectl delete pod -l environment=production\n</code></pre> </li> </ul> <p>Note: Spin up <code>01-pod.yml</code> and <code>02-PodwithLabel.yml</code></p> <p>Common Uses of Labels to Organizing resources, Targeting resources, Filtering and Monitoring and Logging</p>"},{"location":"Kubernetes/Working%20with%20Pods/#resource-requests","title":"Resource requests","text":"<ul> <li>Resource Requests define the minimum amount of CPU or Memory that a container is guaranteed to have.</li> <li> <p>Kubernetes uses the request value to determine how to schedule the container onto a node. The scheduler ensures that the node has at least the requested amount of resources available.</p> <pre><code>kubectl apply -f 04-PodwithResoucesLimit.yml\n</code></pre> <pre><code>kubectl describe pod podwithresourceslimit\n</code></pre> </li> <li> <p>Resource Limits define the maximum amount of CPU or memory that a container is allowed to use.</p> </li> <li> <p>If a container tries to use more than the limit, Kubernetes may throttle the CPU or, in the case of memory, terminate the container.</p> <pre><code>kubectl apply -f 05-PodwithResourceRequestandLimit.yml\n</code></pre> <pre><code>kubectl describe pod podwithresourcesrequestandlimit\n</code></pre> </li> </ul> <p>Example Configuration </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: podwithresourcesrequest\n  labels:\n    name: podwithresourcesrequest\n    environment: production\n    tier: front-end\nspec:\n  containers:\n  - name: podwithresourcesrequest\n    image: nginx\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n</code></pre> <ul> <li>Requests:<ul> <li><code>memory: \"64Mi\"</code> means the container is guaranteed 64 MiB of memory.</li> <li><code>cpu: \"250m\"</code> means the container is guaranteed 250 millicores (0.25 of a CPU core).</li> </ul> </li> <li>Limits:<ul> <li><code>memory: \"128Mi\"</code> means the container can use up to 128 MiB of memory.</li> <li><code>cpu: \"500m\"</code> means the container can use up to 500 millicores (0.5 of a CPU core).</li> </ul> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Pods/#resource-limits","title":"Resource Limits","text":""},{"location":"Kubernetes/Working%20with%20Pods/#best-practices","title":"Best Practices","text":"<ul> <li> <p>Set Requests Based on Baseline Usage: Set resource requests based on the average resource usage of your application. This ensures that the application always gets the resources it needs to function properly.</p> </li> <li> <p>Set Limits Based on Peak Usage: Set resource limits slightly above the expected peak usage to allow for short bursts of resource consumption without risking throttling or OOM kills.</p> </li> <li> <p>Monitor and Adjust: Regularly monitor the resource usage of your applications and adjust the requests and limits as necessary to optimize performance and resource utilization.</p> </li> </ul> <p>Using resource requests and limits effectively ensures that your applications run smoothly, that resources are used efficiently, and that the cluster remains stable and performant.</p>"},{"location":"Kubernetes/Working%20with%20Pods/#multi-container-pod","title":"Multi-Container Pod","text":"<p>Multi-container Pod in Kubernetes is a Pod that runs more than one container. These containers share the same network namespace, storage volumes, and can communicate with each other directly using <code>localhost</code>. Multi-container Pods are typically used when you have tightly coupled processes that need to work together within the same Pod.</p> <p>Why Use Multi-Container Pods?</p> <p>Multi-container Pods are useful in scenarios where:</p> <ol> <li> <p>Sidecar Containers: A container that provides supporting functionality for the main application, such as logging, monitoring, or data synchronization.</p> </li> <li> <p>Ambassador Containers: A container that acts as a proxy or helper for external communication, simplifying interactions with external services.</p> </li> <li> <p>Adapter Containers: A container that modifies or adapts the output of another container, such as transforming logs or data.</p> </li> </ol> <p>Example Configuration </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: multicontainerpod\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    ports:\n    - containerPort: 80\n  - name: sidecar-logger\n    image: busybox\n    command: [\"sh\", \"-c\", \"while true; do echo $(date) - Log from sidecar; sleep 100; done\"]\n</code></pre> <ul> <li> <p>Create Multi-Container pod </p> <pre><code>kubectl apply -f 06-PodwithMultiContainer.yml\n</code></pre> </li> <li> <p>Describe the multi container pod </p> <pre><code>kubectl describe pod multicontainerpod\n</code></pre> </li> <li> <p>Check the container logs </p> <pre><code>kubectl logs pod/multicontainerpod\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Pods/#use-cases","title":"Use Cases","text":"<ul> <li>Microservices: When you have two tightly coupled microservices that need to share resources or communicate frequently.</li> <li>Service Mesh: Sidecar containers are often used in service mesh architectures to handle networking tasks such as load balancing, traffic management, and security.</li> <li>Data Processing Pipelines: Where one container handles data ingestion, another processes the data, and yet another exports the processed data.</li> </ul>"},{"location":"Kubernetes/Working%20with%20Pods/#connect-to-container-in-a-pod","title":"Connect to Container in a POD","text":"<p>To connect to a container within a Kubernetes Pod, you typically use the <code>kubectl exec</code> command. This command allows you to run commands inside a container, effectively giving you access to the container's shell or to execute a specific command inside it.</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- &lt;command&gt;\n</code></pre> <ul> <li><code>-i</code>: Stands for \"interactive.\" It keeps the session open, allowing you to interact with the container.</li> <li><code>-t</code>: Stands for \"TTY.\" It allocates a terminal for the session, which is useful for running interactive commands like a shell.</li> <li><code>&lt;pod-name&gt;</code>: The name of the Pod you want to connect to.</li> <li><code>&lt;command&gt;</code>: The command you want to run inside the container, such as <code>sh</code> or <code>bash</code> for a shell.</li> </ul> <pre><code>kubectl exec -it podwithlabel -- /bin/bash\n</code></pre> <ul> <li> <p>Execute some commands in Nginx container</p> <pre><code>ls\ncd /usr/share/nginx/html\ncat index.html\n</code></pre> </li> <li> <p>Running individual commands in a Container</p> <pre><code>kubectl exec -it podwithlabel env\nkubectl exec -it podwithlabel ls\nkubectl exec -it podwithlabel cat /usr/share/nginx/html/index.html\n</code></pre> </li> <li> <p>Specifying a Container in a Multi-Container Pod</p> <p>If your Pod has multiple containers, you need to specify which container you want to connect to using the <code>-c</code> flag:</p> <pre><code>kubectl exec -it &lt;podname&gt; -c &lt;container-name&gt; -- /bin/bash\n</code></pre> <pre><code>kubectl exec -it multicontainerpod -c nginx-container -- /bin/bash\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Pods/#pod-with-port-number","title":"Pod With Port Number","text":"<p>In Kubernetes, we can specify the port numbers to containers use within a Pod by defining them in the Pod's YAML manifest. This allows you to expose specific ports for your application to handle traffic.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata: \n  name: podwithport\n  labels:\n    name: PodwithPort\n    environment: production\n    tier: front-end\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n</code></pre> <p>Ports: A list of ports that the container will expose. ContainerPort: The port that the container will expose (<code>80</code> for HTTP traffic in this example).</p> <ul> <li> <p>Create pod with container Single port number </p> <pre><code>kubectl apply -f 03-PodwithPort.yml\n</code></pre> </li> <li> <p>Describe the pod </p> <pre><code>kubectl describe pod podwithport\n</code></pre> </li> <li> <p>Create pod with container Multiple port number </p> <pre><code>kubectl apply -f 03.1-PodwithMultiPort.yml\n</code></pre> </li> <li> <p>Describe the pod </p> <pre><code>kubectl describe pod podwithmultiport\n</code></pre> </li> <li> <p>Get the Pod IP </p> <pre><code>kubectl get pod podwithport -o jsonpath='{.status.podIP}'\n</code></pre> </li> <li> <p>Assign POD IP to Variable </p> <pre><code>PodIP=$(kubectl get pod podwithport -o jsonpath='{.status.podIP}')\n</code></pre> <pre><code>echo $PodIP\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Pods/#clean-up","title":"Clean up","text":"<ul> <li> <p>[x] Remove the all pods at a time </p> <pre><code>kubectl delete pods --all \n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20ReplicaSets/","title":"Working With ReplicaSets","text":"<p>ReplicaSets in Kubernetes are a type of controller that ensures a specified number of identical Pod replicas are running at any given time. They help manage the availability of your applications by automatically creating or deleting Pods as necessary to match the desired number of replicas.</p>"},{"location":"Kubernetes/Working%20with%20ReplicaSets/#key-concepts","title":"Key Concepts","text":"<ul> <li>Desired State: The number of Pod replicas that you want running in your cluster.</li> <li>Current State: The number of Pod replicas currently running in your cluster.</li> <li>Self-Healing: If a Pod fails or is deleted, the ReplicaSet controller automatically creates a new Pod to maintain the desired state.</li> </ul>"},{"location":"Kubernetes/Working%20with%20ReplicaSets/#core-components-of-rs","title":"Core Components of RS :","text":"<ul> <li> <p>Specifying Replicas:</p> <ul> <li>The <code>replicas</code> field in the ReplicaSet manifest defines how many replicas of a pod should be running. For example, setting <code>replicas: 4</code> ensures that four instances of the pod are running at all times.</li> <li> <p>Pod Template:</p> </li> <li> <p>The <code>template</code> section of the ReplicaSet defines the pod specification. This includes container images, environment variables, ports, and other configurations.</p> </li> <li> <p>Label Selector:</p> </li> <li> <p>The <code>selector</code> section is used to match the labels on pods. It determines which pods the ReplicaSet is responsible for. This allows the ReplicaSet to manage specific pods based on their labels.</p> </li> <li> <p>Self-Healing:</p> </li> <li> <p>ReplicaSets are self-healing by design. If a pod dies or becomes unresponsive, the ReplicaSet replaces it automatically, ensuring the system remains in the desired state.</p> </li> </ul> </li> </ul> <p>Example Configuration</p> Replicasets/ofl-replicaset.yml<pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: ofl-replicaset\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      environment: production\n  template:\n    metadata:\n      labels: \n        environment: production\n        tier: front-end\n    spec:\n      containers:\n        - name: ofl-webserver\n          image: nginx:latest\n          ports:\n          - containerPort: 80\n</code></pre> <ul> <li> <p>Create ReplicaSet </p> <pre><code>kubectl apply -f Replicasets/ofl-replicaset.yml\n</code></pre> </li> <li> <p>List of the ReplicaSet </p> <pre><code>kubectl get rs\n</code></pre> </li> <li> <p>Describe ReplicaSet </p> <pre><code>kubectl describe replicaset ofl-replicaset\n</code></pre> </li> <li> <p>List out the all objects </p> <pre><code>kubectl get all\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20ReplicaSets/#scaling-replicasets","title":"Scaling ReplicaSets","text":"<ul> <li>Scaling ReplicaSets in Kubernetes involves adjusting the number of pod replicas that are running at any given time. This can be done manually or automatically, depending on your needs.</li> </ul>"},{"location":"Kubernetes/Working%20with%20ReplicaSets/#manual-scaling","title":"Manual Scaling","text":"<p>You can manually scale a ReplicaSet by modifying the <code>replicas</code> field in the ReplicaSet YAML manifest or by using the <code>kubectl</code> command.</p> <ul> <li> <p>Scaling via <code>kubectl</code> Command</p> </li> <li> <p>The easiest way to scale a ReplicaSet is to use the <code>kubectl scale</code> command. This command allows you to specify the desired number of replicas.</p> <pre><code>kubectl scale rs &lt;rs-name&gt; --replicas=5\n</code></pre> <pre><code>kubectl scale rs ofl-replicaset --replicas=5\n</code></pre> </li> <li> <p>Scaling by Editing the YAML Manifest. </p> <ul> <li>You can also scale a ReplicaSet by editing its YAML manifest directly and applying the changes.</li> <li> <p>Update the <code>replicas</code> field in the YAML file using <code>kubectl</code> command</p> <pre><code>kubectl edit rs ofl-replicaset\n</code></pre> </li> <li> <p>Update the <code>replicas</code> field in the YAML file.</p> Replicasets/ofl-replicaset.yml<pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: ofl-replicaset\nspec:\n  replicas: 4 # Change this number to the desired replica count\n  selector:\n    matchLabels:\n      environment: production\n  template:\n    metadata:\n      labels: \n        environment: production\n        tier: front-end\n    spec:\n      containers:\n        - name: ofl-webserver\n          image: nginx:latest\n          ports:\n          - containerPort: 80\n</code></pre> <ul> <li>Apply the updated YAML file:</li> </ul> <pre><code>kubectl apply -f Replicasets/ofl-replicaset.yml\n</code></pre> <ul> <li>Update replica set using <code>sed</code> command </li> </ul> <pre><code>sed -i 's/4/6/g' Replicasets/ofl-replicaset.yml\n</code></pre> </li> </ul> </li> </ul>"},{"location":"Kubernetes/Working%20with%20ReplicaSets/#automatic-scaling","title":"Automatic Scaling","text":"<p>For dynamic scaling based on resource utilization (e.g., CPU, memory), you can use the Horizontal Pod Autoscaler (HPA). The HPA automatically scales the number of pods in a ReplicaSet (or Deployment) based on observed CPU utilization, memory usage, or custom metrics.</p> <pre><code>kubectl autoscale rs ofl-replicaset --min=2 --max=10 --cpu-percent=80\n</code></pre> <ul> <li><code>--min=2</code>: Minimum number of replicas.</li> <li><code>--max=10</code>: Maximum number of replicas.</li> <li><code>--cpu-percent=80</code>: Target average CPU utilization across all pods.</li> </ul> <p>You can check the status of the HPA using:</p> <pre><code>kubectl get hpa\n</code></pre> <p>The HPA will monitor the resource utilization of the pods managed by the ReplicaSet and adjust the number of replicas automatically within the specified range.</p>"},{"location":"Kubernetes/Working%20with%20ReplicaSets/#cleanup","title":"CleanUp","text":"<ul> <li> <p>Delete <code>ofl-replicaset</code></p> <pre><code>kubectl delete -f Replicasets/ofl-replicaset.yml\n</code></pre> <pre><code>kubectl delete hpa ofl-replicaset\n</code></pre> </li> </ul>"},{"location":"Kubernetes/Working%20with%20Services/","title":"Working with Services","text":"<p>As Pods come-and-go (scaling, failures, rollouts etc.), the Service dynamically updates its list of healthy matching Pods. It does this through a combination of label selection and a construct called an Endpoints object.</p> <p>Every time you create a Service, Kubernetes automatically creates an associated Endpoints object. The Endpoints object is used to store a dynamic list of healthy Pods matching the Service\u2019s label selector.</p> <p>Note: Recent versions of Kubernetes are replacing Endpoints objects with more efficient Endpoint slices. The functionality is identical, but Endpoint slices are higher performance and more efficient.</p> <p>Accessing Services from inside the cluster</p> <p>Accessing Services from outside the cluster</p>"},{"location":"Kubernetes/emptyDir/","title":"emptyDir","text":"<p>In Kubernetes, <code>emptyDir</code> is a type of ephemeral storage that provides a temporary directory shared between all containers in a Pod. This directory is created when the Pod is scheduled to a Node and is deleted when the Pod is removed. The <code>emptyDir</code> volume is often used for tasks that require temporary storage, such as caching intermediate files or sharing data between containers in the same Pod.</p>"},{"location":"Kubernetes/emptyDir/#key-aspects","title":"Key Aspects","text":"<ol> <li> <p>Lifecycle: The <code>emptyDir</code>volume is created as soon as the Pod is assigned to a Node and is destroyed when the Pod is deleted. If a container crashes and restarts within the same Pod, the data in the <code>emptyDir</code> persists across container restarts.</p> </li> <li> <p>Storage Medium: By default, <code>emptyDir</code> uses the disk of the Node where the Pod is running. You can also specify that the <code>emptyDir</code>should be memory-backed by setting the medium field to <code>\"Memory\"</code>, which causes the data to be stored in RAM. This can provide faster access but at the cost of using up Node memory.</p> </li> <li> <p>Usage Scenarios:</p> <ol> <li>Scratch Space: Temporary files that do not need to persist beyond the Pod's lifetime.</li> <li>Data Sharing: Sharing files between containers in the same Pod.</li> <li>Caching: Storing intermediate data that can be safely discarded once the Pod is terminated.</li> </ol> </li> </ol>"},{"location":"Kubernetes/emptyDir/#declaratively","title":"Declaratively","text":"<ul> <li> <p>Run the following command to create <code>ofl-empty-dir-pod</code></p> <pre><code>kubectl apply -f ofl-empty-dir-volume-mount.yml\n</code></pre> <ul> <li> </li> </ul> </li> <li> </li> <li> <p>Run the following command to create <code>ofl-empty-dir-shared-volume</code></p> <pre><code>kubectl apply -f ofl-empty-dir-shared-volume-multi-mount.yml\n</code></pre> <ul> <li> </li> </ul> </li> </ul> <p>Note: All containers in a Pod have read/write access to the same emptyDir - if they requested a mount point for it. Containers can access the emptyDir using the same or different mount points.</p>"},{"location":"Kubernetes/emptyDir/#empty-dir-of-single-pod","title":"Empty dir of single pod","text":"ofl-empty-dir-volume-mount.yml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: ofl-empty-dir-pod\nspec:\n    containers:\n    - name: ofl-empty-dir-pod\n      image: busybox\n      command: ['sh', '-c', 'while true; do echo Success! &gt;&gt; /output/output.txt; sleep 10; done']\n      volumeMounts:\n      - name: output-vol\n        mountPath: /output\n    volumes:\n    - name: output-vol\n      hostPath:\n        path: /var/datas\n</code></pre>"},{"location":"Kubernetes/emptyDir/#inspect-empty-dir","title":"Inspect empty dir","text":"<ul> <li>Run the following command to list the pod</li> </ul> <pre><code>kubectl get pod \n</code></pre> <ul> <li>Run the following command to describe the pod <code>ofl-empty-dir-pod</code></li> </ul> <pre><code>kubectl describe pod ofl-empty-dir-pod\n</code></pre> <ul> <li>Run the following command to  deep dig inside the pod analyses the volume behavior</li> </ul> <pre><code>kubectl exec -i -t ofl-empty-dir-pod -- /bin/sh\n</code></pre>"},{"location":"Kubernetes/emptyDir/#empty-dir-multi-container-pod","title":"Empty dir  multi container pod","text":"ofl-empty-dir-shared-volume-multi-mount.yml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ofl-empty-dir-shared-volume\nspec:\n  containers:\n  - name: myvolumes-container-1\n    image: alpine\n    command: ['sh', '-c', 'while true; do echo Success! &gt;&gt; output1.txt; sleep 3600; done']\n\n    volumeMounts:\n    - mountPath: /demo1\n      name: demo-volume\n\n  - name: myvolumes-container-2\n    image: alpine\n    command: ['sh', '-c', 'while true; do echo Success! &gt;&gt; output2.txt; sleep 3600; done']\n\n    volumeMounts:\n    - mountPath: /demo2\n      name: demo-volume\n\n  - name: myvolumes-container-3\n    image: alpine \n    command: ['sh', '-c', 'while true; do echo Success! &gt;&gt; output3.txt; sleep 3600; done']\n\n    volumeMounts:\n    - mountPath: /demo3\n      name: demo-volume\n\n  volumes:\n  - name: demo-volume\n    emptyDir: {}\n</code></pre>"},{"location":"Kubernetes/emptyDir/#inspect-empty-dir_1","title":"Inspect empty dir","text":"<ul> <li>Run the following command to list the pod</li> </ul> <pre><code>kubectl get pod \n</code></pre> <ul> <li>Run the following command to describe the pod <code>ofl-empty-dir-pod</code></li> </ul> <pre><code>kubectl describe pod ofl-empty-dir-shared-volume\n</code></pre> <ul> <li>Run the following command to  deep dig inside the pod analyses the volume behavior</li> </ul> <pre><code>kubectl exec -i -t ofl-empty-dir-shared-volume -c myvolumes-container-1 -- /bin/sh\n</code></pre> <pre><code>kubectl exec -i -t ofl-empty-dir-shared-volume -c myvolumes-container-2 -- /bin/sh\n</code></pre> <pre><code>kubectl exec -i -t ofl-empty-dir-shared-volume -c myvolumes-container-3 -- /bin/sh\n</code></pre>"},{"location":"Kubernetes/emptyDir/#clean-up","title":"Clean up","text":"<ul> <li> <p>Run the following command to clear all deployment in cluster</p> <pre><code>kubectl delete all --all\n</code></pre> </li> </ul>"},{"location":"Terraform/Install%20Terraform/","title":"Install Terraform","text":"<p>Linux</p> <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install terraform\n</code></pre>"},{"location":"Terraform/Intoduction/","title":"What is Terraform?","text":"<p>Terraform is an open-source \u201cInfrastructure as code\u201d software tool originally developed by HashiCorp, that provides a consistent CLI workflow to manage hundreds of cloud services and also enables to automate and manage your infrastructure and platform and services that run on the platform as well.</p> <p>It uses a declarative language, Declarative = define WHAT end result you want.</p> <p>A\u00a0declarative\u00a0coding tool, Terraform enables developers to use a high-level configuration language called HCL (HashiCorp Configuration Language) to describe the desired \u201cend-state\u201d cloud or on-premises infrastructure for running an application. It then generates a plan for reaching that end-state and executes the plan to provision the infrastructure.</p>"},{"location":"Terraform/Intoduction/#what-is-infrastructure-as-code-iac","title":"What is Infrastructure as Code (IaC)?","text":"<p>Infrastructure as Code is the process of provisioning and configuring an environment through code instead of manually setting up the required devices and systems. Once code parameters are defined, developers run scripts, and the IaC platform builds the cloud infrastructure automatically.</p> <p>Such automatic IT setups enable teams to quickly create the desired cloud setting to test and run their software. Infrastructure as Code allows developers to generate any infrastructure component they need, including networks, load balancers, databases, virtual machines, and connection types.</p>"},{"location":"Terraform/Intoduction/#why-infrastructure-as-code-iac","title":"Why Infrastructure as Code (IaC)?","text":"<p>To better understand the advantages of Terraform, it helps to first understand the benefits of\u00a0Infrastructure as Code (IaC).\u00a0IaC allows developers to codify infrastructure in a way that makes provisioning automated, faster, and repeatable.</p> <p>Infrastructure as code can help with the following:</p> <ul> <li>Improve speed:\u00a0Automation is faster than manually navigating an interface when you need to deploy and/or connect resources.</li> <li>Improve reliability:\u00a0If your infrastructure is large, it becomes easy to misconfigure a resource or provision services in the wrong order. With IaC, the resources are always provisioned and configured exactly as declared.</li> <li>Prevent configuration drift:\u00a0Configuration drift occurs when the configuration that provisioned your environment no longer matches the actual environment. (See \u2018Immutable infrastructure\u2019 below.)</li> <li>Support experimentation, testing, and optimization:\u00a0Because Infrastructure as Code makes provisioning new infrastructure so much faster and easier, you can make and test experimental changes without investing lots of time and resources; and if you like the results, you can quickly scale up the new infrastructure for production.</li> </ul>"},{"location":"Terraform/Intoduction/#why-terraform","title":"Why Terraform?","text":"<p>There are a few key reasons developers choose to use Terraform over other Infrastructure as Code tools:</p> <ul> <li>Open source:\u00a0Terraform is backed by large communities of contributors who build plugins to the platform. Regardless of which cloud provider you use, it\u2019s easy to find plugins, extensions, and professional support. This also means Terraform evolves quickly, with new benefits and improvements added consistently.</li> <li>Platform agnostic:\u00a0Meaning you can use it with\u00a0any\u00a0cloud services provider. Most other IaC tools are designed to work with single cloud provider.</li> <li>Immutable infrastructure:\u00a0Most Infrastructure as Code tools create\u00a0mutable\u00a0infrastructure, meaning the infrastructure can change to accommodate changes such as a middleware upgrade or new storage server. The danger with mutable infrastructure is\u00a0configuration drift\u2014as the changes pile up, the actual provisioning of different servers or other infrastructure elements \u2018drifts\u2019 further from the original configuration, making bugs or performance issues difficult to diagnose and correct. Terraform provisions\u00a0immutable (an immutable infrastructure is one that can\u2019t be changed) infrastructure, which means that with each change to the environment, the current configuration is replaced with a new one that accounts for the change, and the infrastructure is reprovisioned. Even better, previous configurations can be retained as versions to enable rollbacks if necessary or desired.</li> </ul>"},{"location":"Tool-integrations/Integrate-docker-with-jenkins/","title":"Docker Integration","text":""},{"location":"Tool-integrations/Integrate-docker-with-jenkins/#integrate-docker-with-jenkins","title":"Integrate Docker with Jenkins","text":""},{"location":"Tool-integrations/SonarQube%20Integration%20with%20Jenkins/","title":"SonarQube Integration with Jenkins","text":""},{"location":"Tool-integrations/SonarQube%20Integration%20with%20Jenkins/#pre","title":"Pre","text":""}]}